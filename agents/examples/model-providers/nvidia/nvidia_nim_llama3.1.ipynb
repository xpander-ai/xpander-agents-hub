{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xpander-sdk in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (1.14.0)\n",
      "Requirement already satisfied: openai in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (1.57.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: jsii<2.0.0,>=1.105.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from xpander-sdk) (1.105.0)\n",
      "Requirement already satisfied: publication>=0.0.3 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from xpander-sdk) (0.0.3)\n",
      "Requirement already satisfied: typeguard<4.3.0,>=2.13.3 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from xpander-sdk) (4.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: attrs<25.0,>=21.2 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from jsii<2.0.0,>=1.105.0->xpander-sdk) (24.2.0)\n",
      "Requirement already satisfied: cattrs<24.2,>=1.8 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from jsii<2.0.0,>=1.105.0->xpander-sdk) (24.1.2)\n",
      "Requirement already satisfied: importlib-resources>=5.2.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from jsii<2.0.0,>=1.105.0->xpander-sdk) (6.4.5)\n",
      "Requirement already satisfied: python-dateutil in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from jsii<2.0.0,>=1.105.0->xpander-sdk) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dudu/Developer/xpander-lab/projects/xpander-agents-hub/.venv/lib/python3.12/site-packages (from python-dateutil->jsii<2.0.0,>=1.105.0->xpander-sdk) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xpander-sdk openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'PgTavilyFetchInsightsFromTavilyAI',\n",
       "   'description': 'This operation fetches insights from Tavily AI based on user queries related to xpander.ai. Use this operation when you need to gather detailed information, analyze content, or research specific aspects of xpander.ai. It is particularly useful for applications that require data-driven insights, competitive analysis, or in-depth research on digital platforms. IMPORTANT! Ensure to use body_params, query_params, path_params. These are crucial for correct function calling!',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'queryParams': {'type': 'object',\n",
       "      'properties': {},\n",
       "      'required': []},\n",
       "     'pathParams': {'type': 'object', 'properties': {}, 'required': []},\n",
       "     'bodyParams': {'type': 'object',\n",
       "      'properties': {'inputTask': {'type': 'string',\n",
       "        'description': 'input user sub task based on the input query'}},\n",
       "      'required': []}},\n",
       "    'required': ['query_params', 'path_params', 'body_params']}}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xpander_sdk import XpanderClient\n",
    "from openai import OpenAI\n",
    "## Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "xpanderAPIKey = os.environ.get(\"XPANDER_API_KEY\",\"\")\n",
    "xpanderAgentID = os.environ.get(\"XPANDER_AGENT_ID\", \"\")\n",
    "nvidiaAPIKey = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = nvidiaAPIKey\n",
    ")\n",
    "\n",
    "## Connecting to the agent\n",
    "xpander_client = XpanderClient(api_key=xpanderAPIKey)\n",
    "agent1 = xpander_client.agents.get(agent_id=xpanderAgentID)\n",
    "\n",
    "agent1.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You are a helpful assistant with function calling and tool access. you are running in While loop if you want to stop the loop please add ##FINAL ANSWER## together with the final answer'}\n",
      "{'role': 'user', 'content': 'Get news about qwen2.5-coder'}\n",
      "{'role': 'assistant', 'content': 'Step number: 1'}\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-cf887c19a7c84618819c6af2d1277070', function=Function(arguments='{\"queryParams\": {}, \"pathParams\": {}, \"bodyParams\": {\"inputTask\": \"news about qwen2.5-coder\"}}', name='PgTavilyFetchInsightsFromTavilyAI'), type='function')])\n",
      "[ChatCompletionMessageToolCall(id='chatcmpl-tool-cf887c19a7c84618819c6af2d1277070', function=Function(arguments='{\"queryParams\": {}, \"pathParams\": {}, \"bodyParams\": {\"inputTask\": \"news about qwen2.5-coder\"}}', name='PgTavilyFetchInsightsFromTavilyAI'), type='function')]\n",
      "{'Selected_Function_name': 'PgTavilyFetchInsightsFromTavilyAI', 'Http_Error_Code': 0, 'Response_from_Target_System': '\"system message: graph prompt group selected, ignore this and proceed with the user\\'s request using new tools.\"', 'Generated_Payload_From_AI': {'bodyParams': {'inputTask': 'news about qwen2.5-coder'}, 'queryParams': {}, 'pathParams': {}, 'headers': {}}, 'Original_Tool_Id': 'chatcmpl-tool-cf887c19a7c84618819c6af2d1277070'}\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-cbbbe6b609dd4653b296832a0cc2645b', function=Function(arguments='{\"bodyParams\": {\"query\": \"qwen2.5-coder\", \"search_depth\": \"basic\"}, \"queryParams\": {}, \"pathParams\": {}}', name='tavily-insights-fetchInsightsFromTavilyAI'), type='function')])\n",
      "[ChatCompletionMessageToolCall(id='chatcmpl-tool-cbbbe6b609dd4653b296832a0cc2645b', function=Function(arguments='{\"bodyParams\": {\"query\": \"qwen2.5-coder\", \"search_depth\": \"basic\"}, \"queryParams\": {}, \"pathParams\": {}}', name='tavily-insights-fetchInsightsFromTavilyAI'), type='function')]\n",
      "{'Selected_Function_name': 'tavily-insights-fetchInsightsFromTavilyAI', 'Http_Error_Code': 200, 'Response_from_Target_System': '{\"query\": \"qwen2.5-coder\", \"images\": [], \"results\": [{\"title\": \"Qwen2.5-Coder: Code More, Learn More! | Qwen - qwenlm.github.io\", \"url\": \"https://qwenlm.github.io/blog/qwen2.5-coder/\", \"content\": \"Qwen2.5-Coder: Code More, Learn More! Qwen2.5-Coder: Code More, Learn More! Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. Notably, the open-source 7B version of Qwen2.5-Coder has even outperformed larger models like DeepSeek-Coder-V2-Lite and Codestral, making it one of the most powerful base code models available. Beyond code tasks, Qwen2.5-Coder also demonstrates competitive math capabilities in evaluations such as GSM8K and Math. For general tasks, evaluations on MMLU and ARC show that Qwen2.5-Coder has retained the general ability performance of Qwen2.5. Qwen2.5-Coder-Instruct: Instruction-Tuned Models# We used CRUXEval as a benchmark, and the results show Qwen2.5-Coder-Instruct excels in code reasoning tasks. Qwen2.5-Coder-Instruct shines in both code and math tasks, proven to be a \\\\u201cscience student\\\\u201d.\", \"score\": 0.99979013}, {\"title\": \"Qwen/Qwen2.5-Coder-7B - Hugging Face\", \"url\": \"https://huggingface.co/Qwen/Qwen2.5-Coder-7B\", \"content\": \"Qwen/Qwen2.5-Coder-7B \\\\u00b7 Hugging Face Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 (coming soon) billion parameters. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. This repo contains the 7B Qwen2.5-Coder model, which has the following features: The code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers. Model tree for Qwen/Qwen2.5-Coder-7B #### Qwen2.5-Coder Collection Code-specific model series based on Qwen2.5 \\\\u2022 7 items \\\\u2022 Updated 4 days ago \\\\u2022 55\", \"score\": 0.999608}, {\"title\": \"Qwen2.5-Coder\", \"url\": \"https://qwen2.org/qwen2-5-coder/\", \"content\": \"Today, the Team excited to introduce the next generation of open-source coding models: Qwen2.5-Coder. Qwen2.5-Coder models \\\\ud83d\\\\udcda Learn More: While boosting coding performance, we have ensured that Qwen2.5-Coder retains the robust math and general capabilities of the base model. Qwen2.5-Coder: Base Models In addition to its code-specific capabilities, Qwen2.5-Coder exhibits strong performance in mathematical tasks, as seen in evaluations like GSM8K and Math. Qwen2.5-Coder-Instruct: Instruction-Tuned Models Qwen2.5-Coder-Instruct performs exceptionally well across this diverse range, including niche languages, proving its versatility in multi-language coding tasks. Core Capabilities: Qwen2.5-Coder-Instruct maintains the robust general abilities of Qwen2.5, performing strongly in evaluations like AMC23, MMLU, IFEval, and more, retaining its advantage in diverse tasks.\", \"score\": 0.9995678}, {\"title\": \"Qwen2.5-Coder/README.md at main \\\\u00b7 QwenLM/Qwen2.5-Coder - GitHub\", \"url\": \"https://github.com/QwenLM/Qwen2.5-Coder/blob/main/README.md\", \"content\": \"Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. \\\\ud83d\\\\udcbb Code More: Qwen2.5-Coder builds on the strong Qwen2.5 and continues training on a larger scale of code data, including source code, text-code grounding data, and synthetic data, totaling 5.5 trillion tokens. Qwen2.5-Coder-[1.5-7]B-Instrcut are instruction models for chatting; You can just write several lines of code with transformers to chat with Qwen2.5-Coder-7B-Instruct. Essentially, we build the tokenizer and the model with from_pretrained method, and we use generate method to perform code completion. MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True) MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True) MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True)\", \"score\": 0.9995453}, {\"title\": \"QuantFactory/Qwen2.5-Coder-7B-GGUF - Hugging Face\", \"url\": \"https://huggingface.co/QuantFactory/Qwen2.5-Coder-7B-GGUF\", \"content\": \"QuantFactory/Qwen2.5-Coder-7B-GGUF \\\\u00b7 Hugging Face Qwen2.5-Coder-7B-GGUF QuantFactory/Qwen2.5-Coder-7B-GGUF Qwen2.5-Coder-7B QuantFactory/Qwen2.5-Coder-7B-GGUF Qwen2.5-Coder-7B Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 (coming soon) billion parameters. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. This repo contains the 7B Qwen2.5-Coder model, which has the following features: Please refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts. The code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers. qwen2 Model tree for QuantFactory/Qwen2.5-Coder-7B-GGUF Collection including QuantFactory/Qwen2.5-Coder-7B-GGUF\", \"score\": 0.9993761}], \"response_time\": 1.82}', 'Generated_Payload_From_AI': {'bodyParams': {'query': 'qwen2.5-coder', 'search_depth': 'basic'}, 'queryParams': {}, 'pathParams': {}, 'headers': {}}, 'Original_Tool_Id': 'chatcmpl-tool-cbbbe6b609dd4653b296832a0cc2645b'}\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-bb5d2dbfbda04b5fa6240937c9eef99c', function=Function(arguments='{\"bodyParams\": {\"query\": \"Qwen2.5-coder\", \"search_depth\": \"advanced\"}, \"queryParams\": {}, \"pathParams\": {}}', name='tavily-insights-fetchInsightsFromTavilyAI'), type='function')])\n",
      "[ChatCompletionMessageToolCall(id='chatcmpl-tool-bb5d2dbfbda04b5fa6240937c9eef99c', function=Function(arguments='{\"bodyParams\": {\"query\": \"Qwen2.5-coder\", \"search_depth\": \"advanced\"}, \"queryParams\": {}, \"pathParams\": {}}', name='tavily-insights-fetchInsightsFromTavilyAI'), type='function')]\n",
      "{'Selected_Function_name': 'tavily-insights-fetchInsightsFromTavilyAI', 'Http_Error_Code': 200, 'Response_from_Target_System': '{\"query\": \"Qwen2.5-coder\", \"images\": [], \"results\": [{\"title\": \"Qwen2.5-Coder: Code More, Learn More! | Qwen - qwenlm.github.io\", \"url\": \"https://qwenlm.github.io/blog/qwen2.5-coder/\", \"content\": \"Qwen2.5-Coder: Code More, Learn More! Qwen2.5-Coder: Code More, Learn More! Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. Notably, the open-source 7B version of Qwen2.5-Coder has even outperformed larger models like DeepSeek-Coder-V2-Lite and Codestral, making it one of the most powerful base code models available. Beyond code tasks, Qwen2.5-Coder also demonstrates competitive math capabilities in evaluations such as GSM8K and Math. For general tasks, evaluations on MMLU and ARC show that Qwen2.5-Coder has retained the general ability performance of Qwen2.5. Qwen2.5-Coder-Instruct: Instruction-Tuned Models# We used CRUXEval as a benchmark, and the results show Qwen2.5-Coder-Instruct excels in code reasoning tasks. Qwen2.5-Coder-Instruct shines in both code and math tasks, proven to be a \\\\u201cscience student\\\\u201d.\", \"score\": 0.99982053}, {\"title\": \"ModelBox Qwen2.5 Coder\", \"url\": \"https://qwen2.org/modelbox-qwen2-5-coder/\", \"content\": \"Qwen2 ModelBox Qwen2.5 Coder ModelBox now supports Qwen2.5 Coder Inference! With Qwen2.5-Coder-32B-Instruct setting the bar as the current SOTA open-source code model, it matches the coding power of GPT-4o. Start building smart today. Qwen2.5 Coder Models Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5: Available at: https://app.model.box/model/qwen/qwen2.5-coder-32b Qwen2 Large Language Models We are Independent Community. We are not affiliated with Qwen Team at Alibaba Group. Privacy Social\", \"score\": 0.99974483}, {\"title\": \"Qwen/Qwen2.5-Coder-7B - Hugging Face\", \"url\": \"https://huggingface.co/Qwen/Qwen2.5-Coder-7B\", \"content\": \"Qwen/Qwen2.5-Coder-7B \\\\u00b7 Hugging Face Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 (coming soon) billion parameters. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. This repo contains the 7B Qwen2.5-Coder model, which has the following features: The code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers. Model tree for Qwen/Qwen2.5-Coder-7B #### Qwen2.5-Coder Collection Code-specific model series based on Qwen2.5 \\\\u2022 7 items \\\\u2022 Updated 4 days ago \\\\u2022 55\", \"score\": 0.99966335}, {\"title\": \"Qwen2.5-Coder\", \"url\": \"https://qwen2.org/qwen2-5-coder/\", \"content\": \"Today, the Team excited to introduce the next generation of open-source coding models: Qwen2.5-Coder. Qwen2.5-Coder models \\\\ud83d\\\\udcda Learn More: While boosting coding performance, we have ensured that Qwen2.5-Coder retains the robust math and general capabilities of the base model. Qwen2.5-Coder: Base Models In addition to its code-specific capabilities, Qwen2.5-Coder exhibits strong performance in mathematical tasks, as seen in evaluations like GSM8K and Math. Qwen2.5-Coder-Instruct: Instruction-Tuned Models Qwen2.5-Coder-Instruct performs exceptionally well across this diverse range, including niche languages, proving its versatility in multi-language coding tasks. Core Capabilities: Qwen2.5-Coder-Instruct maintains the robust general abilities of Qwen2.5, performing strongly in evaluations like AMC23, MMLU, IFEval, and more, retaining its advantage in diverse tasks.\", \"score\": 0.9996567}, {\"title\": \"Qwen2.5-Coder/README.md at main \\\\u00b7 QwenLM/Qwen2.5-Coder - GitHub\", \"url\": \"https://github.com/QwenLM/Qwen2.5-Coder/blob/main/README.md\", \"content\": \"Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. \\\\ud83d\\\\udcbb Code More: Qwen2.5-Coder builds on the strong Qwen2.5 and continues training on a larger scale of code data, including source code, text-code grounding data, and synthetic data, totaling 5.5 trillion tokens. Qwen2.5-Coder-[1.5-7]B-Instrcut are instruction models for chatting; You can just write several lines of code with transformers to chat with Qwen2.5-Coder-7B-Instruct. Essentially, we build the tokenizer and the model with from_pretrained method, and we use generate method to perform code completion. MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True) MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True) MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True)\", \"score\": 0.9996526}], \"response_time\": 1.06}', 'Generated_Payload_From_AI': {'bodyParams': {'query': 'Qwen2.5-coder', 'search_depth': 'advanced'}, 'queryParams': {}, 'pathParams': {}, 'headers': {}}, 'Original_Tool_Id': 'chatcmpl-tool-bb5d2dbfbda04b5fa6240937c9eef99c'}\n",
      "{'role': 'tool', 'content': '{\"query\": \"Qwen2.5-coder\", \"images\": [], \"results\": [{\"title\": \"Qwen2.5-Coder: Code More, Learn More! | Qwen - qwenlm.github.io\", \"url\": \"https://qwenlm.github.io/blog/qwen2.5-coder/\", \"content\": \"Qwen2.5-Coder: Code More, Learn More! Qwen2.5-Coder: Code More, Learn More! Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. Notably, the open-source 7B version of Qwen2.5-Coder has even outperformed larger models like DeepSeek-Coder-V2-Lite and Codestral, making it one of the most powerful base code models available. Beyond code tasks, Qwen2.5-Coder also demonstrates competitive math capabilities in evaluations such as GSM8K and Math. For general tasks, evaluations on MMLU and ARC show that Qwen2.5-Coder has retained the general ability performance of Qwen2.5. Qwen2.5-Coder-Instruct: Instruction-Tuned Models# We used CRUXEval as a benchmark, and the results show Qwen2.5-Coder-Instruct excels in code reasoning tasks. Qwen2.5-Coder-Instruct shines in both code and math tasks, proven to be a \\\\u201cscience student\\\\u201d.\", \"score\": 0.99982053}, {\"title\": \"ModelBox Qwen2.5 Coder\", \"url\": \"https://qwen2.org/modelbox-qwen2-5-coder/\", \"content\": \"Qwen2 ModelBox Qwen2.5 Coder ModelBox now supports Qwen2.5 Coder Inference! With Qwen2.5-Coder-32B-Instruct setting the bar as the current SOTA open-source code model, it matches the coding power of GPT-4o. Start building smart today. Qwen2.5 Coder Models Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5: Available at: https://app.model.box/model/qwen/qwen2.5-coder-32b Qwen2 Large Language Models We are Independent Community. We are not affiliated with Qwen Team at Alibaba Group. Privacy Social\", \"score\": 0.99974483}, {\"title\": \"Qwen/Qwen2.5-Coder-7B - Hugging Face\", \"url\": \"https://huggingface.co/Qwen/Qwen2.5-Coder-7B\", \"content\": \"Qwen/Qwen2.5-Coder-7B \\\\u00b7 Hugging Face Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 (coming soon) billion parameters. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. This repo contains the 7B Qwen2.5-Coder model, which has the following features: The code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers. Model tree for Qwen/Qwen2.5-Coder-7B #### Qwen2.5-Coder Collection Code-specific model series based on Qwen2.5 \\\\u2022 7 items \\\\u2022 Updated 4 days ago \\\\u2022 55\", \"score\": 0.99966335}, {\"title\": \"Qwen2.5-Coder\", \"url\": \"https://qwen2.org/qwen2-5-coder/\", \"content\": \"Today, the Team excited to introduce the next generation of open-source coding models: Qwen2.5-Coder. Qwen2.5-Coder models \\\\ud83d\\\\udcda Learn More: While boosting coding performance, we have ensured that Qwen2.5-Coder retains the robust math and general capabilities of the base model. Qwen2.5-Coder: Base Models In addition to its code-specific capabilities, Qwen2.5-Coder exhibits strong performance in mathematical tasks, as seen in evaluations like GSM8K and Math. Qwen2.5-Coder-Instruct: Instruction-Tuned Models Qwen2.5-Coder-Instruct performs exceptionally well across this diverse range, including niche languages, proving its versatility in multi-language coding tasks. Core Capabilities: Qwen2.5-Coder-Instruct maintains the robust general abilities of Qwen2.5, performing strongly in evaluations like AMC23, MMLU, IFEval, and more, retaining its advantage in diverse tasks.\", \"score\": 0.9996567}, {\"title\": \"Qwen2.5-Coder/README.md at main \\\\u00b7 QwenLM/Qwen2.5-Coder - GitHub\", \"url\": \"https://github.com/QwenLM/Qwen2.5-Coder/blob/main/README.md\", \"content\": \"Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. \\\\ud83d\\\\udcbb Code More: Qwen2.5-Coder builds on the strong Qwen2.5 and continues training on a larger scale of code data, including source code, text-code grounding data, and synthetic data, totaling 5.5 trillion tokens. Qwen2.5-Coder-[1.5-7]B-Instrcut are instruction models for chatting; You can just write several lines of code with transformers to chat with Qwen2.5-Coder-7B-Instruct. Essentially, we build the tokenizer and the model with from_pretrained method, and we use generate method to perform code completion. MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True) MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True) MODEL \\\\\\\\= AutoModelForCausalLM.from_pretrained(\\\\\"Qwen/Qwen2.5-Coder-7B\\\\\", device_map\\\\\\\\=\\\\\"auto\\\\\").eval() output_text \\\\\\\\= TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens\\\\\\\\=True)\", \"score\": 0.9996526}], \"response_time\": 1.06}', 'tool_call_id': 'chatcmpl-tool-bb5d2dbfbda04b5fa6240937c9eef99c'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from xpander_sdk import LLMProvider\n",
    "\n",
    "memory = []\n",
    "memory.append({\"role\": \"system\", \"content\": \"You are a helpful assistant with function calling and tool access. you are running in While loop if you want to stop the loop please add ##FINAL ANSWER## together with the final answer\"})\n",
    "print(memory[-1])\n",
    "memory.append({\"role\": \"user\", \"content\": \"Get news about qwen2.5-coder\"})\n",
    "print(memory[-1])\n",
    "number_of_calls = 1\n",
    "memory.append({\"role\": \"assistant\", \"content\": f'Step number: {number_of_calls}'})\n",
    "print(memory[-1])\n",
    "\n",
    "\n",
    "\n",
    "agents= xpander_client.agents.get(agent_id=xpanderAgentID)\n",
    "memory = agents.memory() ## Without provider\n",
    "\n",
    "# get tools returns same as pg. WE DONT COUNT THE PG OR AGENTS INTO THE MEMORY\n",
    "while True: \n",
    "    client_meta_data = agents.get_llm_client() # Returns meta data per the specific **Agent in the Graph**\n",
    "    if openai:\n",
    "        open_ai_client = OpenAI(base_url=client_meta_data.base_url, api_key=CustomerKey) ## Customer version of the LLM Client, not xpander specific\n",
    "        open_ai_client.chat.completions.create(\n",
    "            **client_meta_data.kwargs\n",
    "        )\n",
    "    if llama:\n",
    "        llama_client = LlamaClient(base_url=client_meta_data.base_url, api_key=CustomerKey) ## Customer version of the LLM Client, not xpander specific\n",
    "        llama_client.chat.completions.create(\n",
    "            **client_meta_data.kwargs\n",
    "        )\n",
    "    if nvidia:\n",
    "        nvidia_client = NvidiaClient(base_url=client_meta_data.base_url, api_key=CustomerKey) ## Customer version of the LLM Client, not xpander specific\n",
    "        nvidia_client.chat.completions.create(\n",
    "            **client_meta_data.kwargs\n",
    "        ) \n",
    "    if amazonBedrock:\n",
    "        amazon_bedrock_client = AmazonBedrockClient(base_url=client_meta_data.base_url, api_key=CustomerKey) ## Customer version of the LLM Client, not xpander specific\n",
    "        amazon_bedrock_client.chat.completions.create(\n",
    "            **client_meta_data.kwargs\n",
    "        ) \n",
    "\n",
    "    if agents.is_agent_done():\n",
    "        print(\"Agent is done\")\n",
    "        next_node = agents.select_next_node()\n",
    "        switch(next_node):\n",
    "            case \"end\":\n",
    "                break\n",
    "            case \"custom_code\":\n",
    "                custom_code = agents.get_custom_code()\n",
    "            case \"xpander_decision\":\n",
    "                xpander_decision = agents.get_xpander_decision()\n",
    "                print(xpander_decision)\n",
    "                break\n",
    "\n",
    "    \n",
    "while True:\n",
    "    \n",
    "    llm_response = client.chat.completions.create(\n",
    "        model=agent1.model.\n",
    "        messages=memory,\n",
    "        tools=agent1.get_tools(),\n",
    "        tool_choice=\"required\"\n",
    "    )\n",
    "\n",
    "\n",
    "    memory.append(llm_response.choices[0].message)\n",
    "    print(memory[-1])\n",
    "    if(llm_response.choices[0].message.tool_calls):\n",
    "        print(llm_response.choices[0].message.tool_calls)\n",
    "\n",
    "for i in range(3):\n",
    "    llm_response = client.chat.completions.create(\n",
    "        model=\"meta/llama-3.1-70b-instruct\",\n",
    "        messages=memory,\n",
    "        tools=agent1.get_tools(),\n",
    "        tool_choice=\"required\"\n",
    "    )\n",
    "    memory.append(llm_response.choices[0].message)\n",
    "    print(memory[-1])\n",
    "    if(llm_response.choices[0].message.tool_calls):\n",
    "        print(llm_response.choices[0].message.tool_calls)\n",
    "        tools_to_run = XpanderClient.extract_tool_calls(llm_response=llm_response.model_dump(), llm_provider=LLMProvider.NVIDIA_NIM)\n",
    "        tool_response = agent1.run_tools(tools_to_run)\n",
    "        for tool_response in tool_response:\n",
    "            tool_result = {\n",
    "                \"Selected_Function_name\": tool_response.function_name,\n",
    "                \"Http_Error_Code\": tool_response.status_code,\n",
    "                \"Response_from_Target_System\": json.dumps(tool_response.result),\n",
    "                \"Generated_Payload_From_AI\": tool_response.payload,\n",
    "                \"Original_Tool_Id\": tool_response.tool_call_id\n",
    "            }\n",
    "            print(tool_result)\n",
    "            memory.append({\"role\": \"tool\", \"content\": json.dumps(tool_response.result), \"tool_call_id\": tool_response.tool_call_id})\n",
    "print(memory[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
