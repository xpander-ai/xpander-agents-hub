{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from xpander_sdk import XpanderClient, LLMProvider, OpenAISupportedModels\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Setup Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "openai_client = OpenAI(api_key= os.environ.get(\"OPENAI_API_KEY\", \"\"))\n",
    "xpander_client = XpanderClient(api_key=os.environ.get(\"XPANDER_API_KEY\", \"\"))\n",
    "xpander_agent = xpander_client.agents.get(agent_id=os.environ.get(\"XPANDER_AGENT_ID\", \"\"))\n",
    "\n",
    "class SharedMemory:\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def add_message(self, content: str, role: str = \"assistant\", agent_name: str = None):\n",
    "        self.memory.append({\"role\": role, \"content\": content, \"agent_name\": agent_name})\n",
    "\n",
    "    def get_memory(self) -> list:\n",
    "        return self.memory\n",
    "\n",
    "\n",
    "class PlannerAgent:\n",
    "    def __init__(self, handler, tools: list, task_message: str, system_message: str, shared_memory: SharedMemory,\n",
    "                 finish_message: str = \"Final Answer\"):\n",
    "        self.handler = handler\n",
    "        self.tools = tools\n",
    "        self.task_message = task_message\n",
    "        self.system_message = system_message\n",
    "        self.local_memory = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": task_message},\n",
    "        ]\n",
    "        self.finish_message = finish_message\n",
    "        self.is_finished = False\n",
    "        self.step_number = 1\n",
    "    def invoke_llm(self, memory, tools=None, model=OpenAISupportedModels.GPT_4_O, max_tokens=16384):\n",
    "        try:\n",
    "            response = self.handler.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=memory,\n",
    "                tools=tools,\n",
    "                tool_choice=\"none\",\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.0,\n",
    "                top_p=1\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error invoking LLM: {e}\")\n",
    "        \n",
    "    def run_post_processing(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        Post-process the LLM response to check for completion.\n",
    "        \"\"\"\n",
    "        if re.search(self.finish_message, response):\n",
    "            self.is_finished = True \n",
    "        return response\n",
    "\n",
    "    def finished(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the agent has completed its task.\n",
    "        \"\"\"\n",
    "        return self.is_finished\n",
    "\n",
    "\n",
    "\n",
    "class ToolSelectorAgent:\n",
    "    def __init__(self, handler, tools: list, task_message: str, system_message: str, shared_memory: SharedMemory):\n",
    "        self.handler = handler\n",
    "        self.tools = tools\n",
    "        self.task_message = task_message\n",
    "        self.system_message = system_message\n",
    "        self.local_memory = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": task_message},\n",
    "        ]\n",
    "        self.selected_tools = []\n",
    "\n",
    "    def invoke_llm(self, memory, tools=None, model=OpenAISupportedModels.GPT_4_O, max_tokens=16384):\n",
    "        try:                                \n",
    "            response=self.handler.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=memory,\n",
    "                tools=tools,\n",
    "                parallel_tool_calls=False,\n",
    "                tool_choice=\"required\",\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.0,\n",
    "                top_p=1\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error invoking LLM: {e}\")\n",
    "\n",
    "class ParserAgent:\n",
    "    def __init__(self, handler, tools: list, task_message: str, system_message: str, shared_memory: SharedMemory):\n",
    "        self.handler = handler\n",
    "        self.task_message = task_message\n",
    "        self.system_message = system_message\n",
    "        self.tools = tools\n",
    "        self.local_memory = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": task_message},\n",
    "        ]\n",
    "    def invoke_llm(self, memory, tools=None, model=OpenAISupportedModels.GPT_4_O, max_tokens=16384):\n",
    "        try:\n",
    "            response=self.handler.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=memory,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error invoking LLM ParserAgent: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_planner = '''As a planner agent your task is to breakdown the main task into sub tasks that fulfill the main task. \n",
    "your main task is to build a comprehensive report on the topic provided by the user '{user_query}' '''\n",
    "\n",
    "tool_selector_system_prompt = '''You are an tool selector agent responsible for selecting the correct tool with the most relevant parameters that will fullfil the task your will received\n",
    "from the planner agent to execute the plan.'''\n",
    "\n",
    "parser_system_prompt = '''You are a parser agent that get to get the tool execution response from the relevant tool and generate a human readable report about the required\n",
    "user request/topic '{user_query}' based on the tool response and the plan in this current step.'''\n",
    "\n",
    "planner_task_prompt = '''\n",
    "As a planner research agent your task is to plan the the full workflow for the tool execution agent and the parser agent for making a comprehensive report on the topic provided by the user '{user_query}'\n",
    "you need to explain each step in the workflow, what is the current required data and what are the expected results.\n",
    "\n",
    "what expected from you:\n",
    " \n",
    "Produce a high-quality, comprehensive report that addresses the user’s query in-depth, using the latest and most relevant information gathered from multiple sources. The report should be informative, fact-based, and structured cohesively, with clear sections.\n",
    "Available Tools for Data Gathering:\n",
    "You have access to the following tools to collect data on the topic:\n",
    "   - **Tavily**: conduct a search on the topic and get a summary of the relevant information.\n",
    "   - **Arxiv**: Fetch recent articles within the last year on the topic focusing on their introductory sections.\n",
    "   - **Perplexity**: Gather additional, complementary data that may not appear in Tavily or Arxiv results.\n",
    "   - **LinkedIn**: Search for posts from professionals and organizations that provide real-world insights and can enrich the report.\n",
    "Use each tool to gather data relevant to the user’s query. after all tools are used you must create a PDF report using the Markdown-to-PDF tool the expected output is a link to the PDF report.\n",
    "\n",
    "These are the strict rules:\n",
    "1. Always provide your plan in natural language, ensuring it is closely related to the input tools, you must related to the available tools you got.\n",
    "2. Be specific in the plan and explain what should be the results of this step after parsing the API response. \n",
    "3. User's query can't be fulfilled without at least one 'Parser Response'. You can't fulfill task without at least one API calling and response.\n",
    "4. If the query has not been fulfilled, explain how to fix the last step and continue to output your plan.\n",
    "5. Return only the next Plan step (i+1) you generated and do not mention all the steps list until now. you'll get the conversation history Plan steps [1,...,i-1] and API responses after parsing, you will use it to generate the next step.\n",
    "6. If the the API request failed, return how you recommend to handle this error in the next retry of the tool calling.\n",
    "7. you must return only one step in each call! never return the full step pipeline in one iteration.\n",
    "8. after you collect all the information you must create the final PDF report using the Markdown-to-PDF tool.\n",
    "9. You MUST return the Final Answer after using all the tools: Tavily, Arxiv, Perplexity and LinkedIn and Markdown-to-PDF tool !\n",
    "\n",
    "this is the expected output template:\n",
    "if the query has not been fulfilled:\n",
    "Plan step (i+1): [the next step of your plan for how to solve the query].\n",
    "\n",
    "if all the data is collected and 'Markdown-to-PDF' tool is used and the Parser returned link to the PDF report you MUST return the final answer with the link to the PDF report that got from the Parser.\n",
    "you MUST return by the following template: Final Answer: [link to the PDF report].\n",
    "\n",
    "please start with the first step and return only one step in each call!\n",
    "your research on the topic {user_query} begin now...\n",
    "'''\n",
    "\n",
    "tool_selector_task_prompt = '''\n",
    "as a part of a multi agent system your task is to get as much information as possible on the user topic {user_query} and create a comprehensive PDF report.\n",
    "\n",
    "Your task:\n",
    "1. Select the most accurate tool to fulfill the current task provided by the planner agent pay attention to the tools the planner agent provided you. \n",
    "2. Generate all required parameters by the schema you got that will fulfill the task.\n",
    "3. You must return your answer as a tool_call with the function name and relevant arguments.\n",
    "4. If the planer explain about the error and how to fix it, you should fix the last tool call parameters and return the new tool call.\n",
    "5. when you creating the final report you must used all the previews information that collected in the parserAgent response steps.\n",
    "6. if tool call is success you must use each tool only once!\n",
    "here the required user topic: {user_query}\n",
    "'''\n",
    "\n",
    "parser_task_prompt = '''\n",
    "as part of the multi agent pipeline you are the parser agent and your task is to get the current step plan and the response from the tool that executed for this plan and parse this response to fulfill the plan.\n",
    "your response should contains all the relevent details for the report without missing information.\n",
    "your response should contains as much details as you can for creating the final report.\n",
    "the report must be informative and clear as much as you can and stick with the plan.\n",
    "\n",
    "These are the strict rules:\n",
    "1. when you creating the final report you must use all the information you got until now on the user topic {user_query}.\n",
    "2. you must return the report as human readable text that include all the information.\n",
    "3. Confirm that the report comprehensively answers the user’s query and provides an in-depth view based on the gathered data.\n",
    "4. Verify that all sections flow logically, contributing to a unified, fact-based narrative without tool-specific attributions.\n",
    "5. Ensure that the report includes a structured introduction and conclusion, and that it is cohesive, informative, and thoroughly addresses the main topic.\n",
    "6. Do NOT add your own words to the report!  you must use only the information you got from the tools.\n",
    "7. Ensure that the report is formatted correctly and is easy to read and the main title is bold and larger than the other titles.\n",
    "8. after you use 'Markdown-to-PDF' tool you must tell the planner agent that you finished and return only the link to the PDF report that got from the tool. DO NOT RETURN ANYTHING ELSE!\n",
    "\n",
    "here the required user topic: {user_query}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Query\n",
    "user_query = \"create a report on AI Agents\"\n",
    "\n",
    "start_time = time.time()\n",
    "tools = xpander_agent.get_tools()\n",
    "current_date = datetime.now().date()\n",
    "shared_memory = SharedMemory()\n",
    "\n",
    "# Initialize Agents\n",
    "planner_agent = PlannerAgent(handler=openai_client,tools=tools, task_message=planner_task_prompt.format(user_query=user_query), system_message=system_prompt_planner.format(user_query=user_query),shared_memory=shared_memory)\n",
    "tool_selector_agent = ToolSelectorAgent(handler=openai_client,tools=tools,task_message=tool_selector_task_prompt.format(user_query=user_query),system_message=tool_selector_system_prompt , shared_memory=shared_memory)\n",
    "parser_agent = ParserAgent(handler=openai_client, tools=tools, task_message=parser_task_prompt.format(user_query=user_query) ,system_message=parser_system_prompt.format(user_query=user_query), shared_memory=shared_memory)\n",
    "\n",
    "\n",
    "planner_response = planner_agent.invoke_llm(memory=planner_agent.local_memory,tools=planner_agent.tools)\n",
    "shared_memory.add_message(planner_response.choices[0].message.content,role=\"assistant\",agent_name=\"PlannerAgent\")\n",
    "logger.info(\"Planner Response: %s\", planner_response.choices[0].message.content)\n",
    "\n",
    "tool_selector_response = tool_selector_agent.invoke_llm(memory=tool_selector_agent.local_memory+shared_memory.get_memory(),tools=tools)\n",
    "tool_calls = xpander_client.extract_tool_calls(llm_response=tool_selector_response.model_dump(), llm_provider=LLMProvider.OPEN_AI)\n",
    "_ = xpander_agent.run_tools(tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_memory = SharedMemory()\n",
    "tools = xpander_agent.get_tools()\n",
    "\n",
    "\n",
    "planner_response = planner_agent.invoke_llm(memory=planner_agent.local_memory,tools=tools)\n",
    "shared_memory.add_message(planner_response.choices[0].message.content,role=\"assistant\",agent_name=\"PlannerAgent\")\n",
    "logger.info(\"Planner Response: %s\", planner_response.choices[0].message.content)\n",
    "\n",
    "while not planner_agent.finished():\n",
    "    try:\n",
    "        \n",
    "        tool_selector_response = tool_selector_agent.invoke_llm(memory=tool_selector_agent.local_memory+shared_memory.get_memory(),tools=tools)\n",
    "        tool_calls = xpander_client.extract_tool_calls(llm_response=tool_selector_response.model_dump(), llm_provider=LLMProvider.OPEN_AI)\n",
    "        if tool_calls:\n",
    "            for tool_call in tool_calls:\n",
    "                logger.info(\"Tool calls: %s\", tool_call.name)\n",
    "                tool_response = xpander_agent.run_tool(tool_call)\n",
    "                logger.info(\"Tool response: %s\", tool_response.result)\n",
    "                \n",
    "                selected_tool_params = json.dumps(tool_selector_response.model_dump()['choices'][0]['message']['tool_calls'])\n",
    "                selected_tool_data = {\n",
    "                    \"selected_tool\": tool_call.name,\n",
    "                    \"tool_call_id\": tool_call.tool_call_id,\n",
    "                    \"params\": selected_tool_params,\n",
    "                    \"response\": tool_response.result\n",
    "                }\n",
    "                logger.info(\"Tool selector response: %s\", json.dumps(selected_tool_data))\n",
    "                \n",
    "                parser_message = (\n",
    "                    f\"Current Task: {planner_response.choices[0].message.content}\\n\"\n",
    "                    f\"The selected Tool: {tool_call.name}\\n\"\n",
    "                    f\"The params Tool: {json.dumps(selected_tool_data)}\\n\"\n",
    "                )\n",
    "\n",
    "                parser_response = parser_agent.invoke_llm(memory=parser_agent.local_memory+shared_memory.get_memory()+[{\"role\": \"user\", \"content\": parser_message}],tools=tools)\n",
    "                shared_memory.add_message(parser_response.choices[0].message.content,role=\"assistant\",agent_name=\"ParserAgent\")\n",
    "                logger.info(\"Parser response: %s\", parser_response.choices[0].message.content)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline error: {e}\")\n",
    "        break\n",
    "    \n",
    "    tools = xpander_agent.get_tools()\n",
    "    planner_response = planner_agent.invoke_llm(memory=planner_agent.local_memory+shared_memory.get_memory(),tools=tools)\n",
    "    shared_memory.add_message(planner_response.choices[0].message.content,role=\"assistant\",agent_name=\"PlannerAgent\")\n",
    "    logger.info(\"Planner response: %s\", planner_response.choices[0].message.content)\n",
    "    \n",
    "    planner_agent.run_post_processing(planner_response.choices[0].message.content)\n",
    "    \n",
    "\n",
    "logger.info(\"Pipeline execution complete!\")\n",
    "logger.info(\"Shared Memory:\")\n",
    "logger.info(shared_memory.get_memory())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
